{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9694017,"sourceType":"datasetVersion","datasetId":5926950},{"sourceId":9695710,"sourceType":"datasetVersion","datasetId":5928262},{"sourceId":9700738,"sourceType":"datasetVersion","datasetId":5932088}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing relevant libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.functional as F\nimport torch.nn as nn\n\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom pprint import pprint\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:34.402188Z","iopub.execute_input":"2024-10-24T23:58:34.402560Z","iopub.status.idle":"2024-10-24T23:58:34.414742Z","shell.execute_reply.started":"2024-10-24T23:58:34.402525Z","shell.execute_reply":"2024-10-24T23:58:34.413608Z"},"trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"torch.__version__","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:34.419155Z","iopub.execute_input":"2024-10-24T23:58:34.419723Z","iopub.status.idle":"2024-10-24T23:58:34.427993Z","shell.execute_reply.started":"2024-10-24T23:58:34.419684Z","shell.execute_reply":"2024-10-24T23:58:34.427012Z"},"trusted":true},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"'2.4.0'"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:34.429688Z","iopub.execute_input":"2024-10-24T23:58:34.430332Z","iopub.status.idle":"2024-10-24T23:58:34.441883Z","shell.execute_reply.started":"2024-10-24T23:58:34.430295Z","shell.execute_reply":"2024-10-24T23:58:34.440925Z"},"trusted":true},"outputs":[],"execution_count":68},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:34.443573Z","iopub.execute_input":"2024-10-24T23:58:34.443861Z","iopub.status.idle":"2024-10-24T23:58:34.453670Z","shell.execute_reply.started":"2024-10-24T23:58:34.443831Z","shell.execute_reply":"2024-10-24T23:58:34.452704Z"},"trusted":true},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":69},{"cell_type":"markdown","source":"# Filtering the text","metadata":{}},{"cell_type":"code","source":"from os import linesep\nimport string\n\n# Read the file\nfile_path = '/kaggle/input/text-for-next-word-predictor/leo tolstoy - war and peace.txt'\n\n# Open and read the contents of the file\nwith open(file_path, 'r', encoding='utf-8') as file:\n    text = file.read()\n\nfiltered_text = re.sub(r'-', ' ', text)\nfiltered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\nfiltered_text = filtered_text.lower()\n\nlines=filtered_text.split(\".\")\nwords=['.']\nfor l in lines:\n    for w in l.split():\n        if (len(w)>0):\n            words.append(w)\nwords=set(words)\n\nprint(\"Total no. of lines: \", len(lines))\nprint(\"Total unique words: \", len(words))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-24T23:58:34.455781Z","iopub.execute_input":"2024-10-24T23:58:34.456449Z","iopub.status.idle":"2024-10-24T23:58:34.821748Z","shell.execute_reply.started":"2024-10-24T23:58:34.456402Z","shell.execute_reply":"2024-10-24T23:58:34.820722Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total no. of lines:  30588\nTotal unique words:  17877\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"#Mapping from words to integers and vice versa\nstoi={s:i for i,s in enumerate(words)}\nitos={i:s for s,i in stoi.items()}\nprint(len(itos))","metadata":{"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-24T23:58:34.823853Z","iopub.execute_input":"2024-10-24T23:58:34.824625Z","iopub.status.idle":"2024-10-24T23:58:34.841264Z","shell.execute_reply.started":"2024-10-24T23:58:34.824571Z","shell.execute_reply":"2024-10-24T23:58:34.840173Z"},"trusted":true},"outputs":[{"name":"stdout","text":"17877\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"# Generating the labelled dataset","metadata":{}},{"cell_type":"code","source":"# Hyperparameter\nblock_size=5 # context_length: how many words do we take to predict the next one\n\n# X and Y matrices to store the data for training\n# X stores the half lines\n# Y stores the next word\nX,Y=[],[]\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor l in lines:\n  context=[0]*block_size\n  word_l=l.split()\n\n  for i in range(len(word_l)):\n    ix=stoi[word_l[i]]\n    X.append(context)\n    Y.append(ix)\n    # print(' '.join(itos[i] for i in context), '--->', itos[ix])\n    context = context[1:] + [ix]\n\n    if (i==len(word_l)-1):\n        ix=stoi['.']\n        X.append(context)\n        Y.append(ix)\n        # print(' '.join(itos[i] for i in context), '--->', itos[ix])\n        context = context[1:] + [ix]\n\n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n\nX.shape, Y.shape, X.dtype, Y.dtype","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-24T23:58:34.842603Z","iopub.execute_input":"2024-10-24T23:58:34.842978Z","iopub.status.idle":"2024-10-24T23:58:37.793538Z","shell.execute_reply.started":"2024-10-24T23:58:34.842941Z","shell.execute_reply":"2024-10-24T23:58:37.792621Z"},"trusted":true},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(torch.Size([592621, 5]), torch.Size([592621]), torch.int64, torch.int64)"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"# Defining the model","metadata":{}},{"cell_type":"code","source":"emb_dim = 64 # Hyperparameter\n\n# Embedding layer\nemb=torch.nn.Embedding(len(stoi),emb_dim).to(device)\nprint(emb)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-24T23:58:37.795903Z","iopub.execute_input":"2024-10-24T23:58:37.796235Z","iopub.status.idle":"2024-10-24T23:58:37.813730Z","shell.execute_reply.started":"2024-10-24T23:58:37.796202Z","shell.execute_reply":"2024-10-24T23:58:37.812723Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Embedding(17877, 64)\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"class Next_Word_Predictor(nn.Module):\n    def __init__(self, block_size, vocab_size, emb_dim, hidden_dim, activation_fn, seed_value):\n        super().__init__()\n        self.block_size = block_size\n        self.hyperparams = {'block_size':self.block_size, 'emb_dim':emb_dim, 'hidden_dim':hidden_dim, 'activation_fn':activation_fn, 'seed_value':seed_value}\n        self.emb = nn.Embedding(vocab_size, emb_dim)\n        self.linear1 = nn.Linear(block_size * emb_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n         \n        if activation_fn == 'sigmoid':\n            self.activation = torch.sigmoid  \n        else:\n            self.activation = torch.relu \n\n    def forward(self, x):\n        # Embedding layer\n        x = self.emb(x)\n        x = x.view(x.shape[0], -1)  \n        \n        # Hidden layer\n        x = self.linear1(x)\n        x = self.activation(x)\n        \n        # Output layer\n        x = self.linear2(x)\n        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:37.815066Z","iopub.execute_input":"2024-10-24T23:58:37.815559Z","iopub.status.idle":"2024-10-24T23:58:37.824976Z","shell.execute_reply.started":"2024-10-24T23:58:37.815514Z","shell.execute_reply":"2024-10-24T23:58:37.824077Z"},"trusted":true},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"def train_model(X, Y, block_size, emb_dim, vocab_size, hidden_dim, activation_fn, seed_value, device, batch_size=1024, epochs=201, print_every=20):\n    \"\"\"\n    Train the model with the specified seed value.\n    \n    Arguments:\n    X -- training data (input features)\n    Y -- training data (labels)\n    block_size -- context size for input sequence\n    emb_dim -- embedding dimension for the model\n    vocab_size -- the size of the vocabulary\n    hidden_dim -- the size of the hidden layer\n    activation_fn -- the activation function to use ('relu', 'tanh', 'sigmoid')\n    seed_value -- the seed value for reproducibility\n    device -- device to run the training on ('cpu' or 'cuda')\n    batch_size -- the size of each mini-batch (default: 1024)\n    epochs -- number of training epochs (default: 2000)\n    print_every -- print loss after every 'n' epochs (default: 10)\n    \"\"\"\n    \n    torch.manual_seed(seed_value)\n\n    model = Next_Word_Predictor(block_size, vocab_size, emb_dim, hidden_dim, activation_fn, seed_value).to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n\n    for epoch in range(epochs):\n        \n        # Mini-batch training\n        for i in range(0, X.shape[0], batch_size):\n            x = X[i:i + batch_size].to(device)\n            y = Y[i:i + batch_size].to(device)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n            \n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        \n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}: Loss = {loss.item()}')\n    \n    return model\n","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-10-24T23:58:37.826019Z","iopub.execute_input":"2024-10-24T23:58:37.826330Z","iopub.status.idle":"2024-10-24T23:58:37.841637Z","shell.execute_reply.started":"2024-10-24T23:58:37.826300Z","shell.execute_reply":"2024-10-24T23:58:37.840755Z"},"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"code","source":"vocab_size = len(stoi)\n# Some other hyperparameters\nhidden_dim = 1024\nactivation_fn = 'relu' \nseed_value = 42 ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:37.843663Z","iopub.execute_input":"2024-10-24T23:58:37.844321Z","iopub.status.idle":"2024-10-24T23:58:37.855354Z","shell.execute_reply.started":"2024-10-24T23:58:37.844277Z","shell.execute_reply":"2024-10-24T23:58:37.854517Z"},"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"code","source":"model = train_model(X, Y, block_size, emb_dim, vocab_size, hidden_dim, activation_fn, seed_value, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:58:37.856334Z","iopub.execute_input":"2024-10-24T23:58:37.856651Z","iopub.status.idle":"2024-10-25T01:09:21.219522Z","shell.execute_reply.started":"2024-10-24T23:58:37.856619Z","shell.execute_reply":"2024-10-25T01:09:21.218548Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 0: Loss = 6.1316423416137695\nEpoch 20: Loss = 1.0935217142105103\nEpoch 40: Loss = 0.6827052235603333\nEpoch 60: Loss = 0.5356357097625732\nEpoch 80: Loss = 0.4648418724536896\nEpoch 100: Loss = 0.41502639651298523\nEpoch 120: Loss = 0.39065784215927124\nEpoch 140: Loss = 0.3561030328273773\nEpoch 160: Loss = 0.35712406039237976\nEpoch 180: Loss = 0.3540221154689789\nEpoch 200: Loss = 0.34291714429855347\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Saving the model\n\ntorch.save(model, 'model_variant_1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-25T02:49:31.997444Z","iopub.execute_input":"2024-10-25T02:49:31.998090Z","iopub.status.idle":"2024-10-25T02:49:32.333642Z","shell.execute_reply.started":"2024-10-25T02:49:31.998048Z","shell.execute_reply":"2024-10-25T02:49:32.332431Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Saving the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_variant_1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Generating Predictions","metadata":{}},{"cell_type":"code","source":"# Generate names from trained model\n\ndef generate_next_words(model, itos, stoi, content, seed_value, k, max_len=10):\n    torch.manual_seed(seed_value)\n    \n    block_size = model.block_size\n    context = content.lower()\n    context = re.sub('[^a-zA-Z0-9 \\.]', '', context)\n    context = re.sub('\\.', ' . ', context)\n    word_c = context.split()\n    context = []\n    for i in range(len(word_c)):\n        try:\n            if stoi[word_c[i]]:\n                context.append(word_c[i])\n        except:\n            continue\n            \n    context = [stoi[w] for w in context]\n               \n    if len(context) <= block_size:\n        context = [0] * (block_size - len(context)) + context\n    elif len(context) > block_size:\n        context = context[-block_size:]\n\n    for i in range(k):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        logits = y_pred\n        \n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        word = itos[ix]\n        content += \" \" + word\n        context = context [1:] + [ix]\n        \n    return content\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.407728Z","iopub.execute_input":"2024-10-25T01:09:21.408058Z","iopub.status.idle":"2024-10-25T01:09:21.418959Z","shell.execute_reply.started":"2024-10-25T01:09:21.408024Z","shell.execute_reply":"2024-10-25T01:09:21.418051Z"},"trusted":true},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Generate names from trained model\n\npara=\"\"\ncontent=input(\"Enter some content: \")\nk=int(input(\"Enter no. of words to be generated: \"))\npara+=generate_next_words(model, itos, stoi, content, seed_value, k)\npara+=\"\\n\\n\"\nprint(para)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.420158Z","iopub.execute_input":"2024-10-25T01:09:21.420705Z","iopub.status.idle":"2024-10-25T01:09:21.438079Z","shell.execute_reply.started":"2024-10-25T01:09:21.420661Z","shell.execute_reply":"2024-10-25T01:09:21.436889Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[80], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    content\"he is eating\"\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2371000541.py, line 5)","output_type":"error"}],"execution_count":80},{"cell_type":"markdown","source":"# Visualization of embeddings","metadata":{}},{"cell_type":"code","source":"embedding_weights = model.emb.weight.detach().cpu().numpy() #to be used for visualization","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.438802Z","iopub.status.idle":"2024-10-25T01:09:21.439172Z","shell.execute_reply.started":"2024-10-25T01:09:21.438973Z","shell.execute_reply":"2024-10-25T01:09:21.438990Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Set the number of clusters (you can experiment with this number)\nn_clusters = 10\n\n# Perform K-means clustering on the embeddings\nkmeans = KMeans(n_clusters=n_clusters, random_state=1)\nclusters = kmeans.fit_predict(embedding_weights)\n\n# Create a dictionary to store words grouped by cluster\nclustered_words = {i: [] for i in range(n_clusters)}\n\n# Assign words to their respective clusters\nfor word, idx in stoi.items():\n    if idx < embedding_weights.shape[0]:\n        cluster = clusters[idx]\n        clustered_words[cluster].append(word)\n\n# Print words in each cluster\nfor cluster, words in clustered_words.items():\n    print(f\"Cluster {cluster}: {', '.join(words[:10])}\")  # Limiting to first 10 words for readability","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.440567Z","iopub.status.idle":"2024-10-25T01:09:21.440974Z","shell.execute_reply.started":"2024-10-25T01:09:21.440761Z","shell.execute_reply":"2024-10-25T01:09:21.440779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport random\nimport torch\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Group words based on their suffixes\ndef group_words(itos):\n    groups = {\n        'verbs_ing': [],\n        'verbs_ed': [],\n        'adverbs_ly': [],\n        'nouns': [],\n        'adjectives': [],\n        'pronouns': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'],\n    }\n\n    # Iterate over all words\n    for word in itos.values():\n        # Verbs ending with 'ing'\n        if word.endswith('ing'):\n            groups['verbs_ing'].append(word)\n        # Verbs ending with 'ed'\n        elif word.endswith('ed'):\n            groups['verbs_ed'].append(word)\n        # Adverbs ending with 'ly'\n        elif word.endswith('ly'):\n            groups['adverbs_ly'].append(word)\n        # Nouns: words ending with common noun suffixes\n        elif word.endswith('ness') or word.endswith('ment') or word.endswith('tion'):\n            groups['nouns'].append(word)\n        # Adjectives: words ending with common adjective suffixes\n        elif word.endswith('able') or word.endswith('ous') or word.endswith('ive'):\n            groups['adjectives'].append(word)\n\n    return groups\n\n# Get the grouped words\ngrouped_words = group_words(itos)\n\n# Select a subset of words for each group (e.g., 10 from each group)\nnum_words_per_group = 10000\nselected_words = []\nselected_group_labels = []\n\ncolors = {\n    'verbs_ing': 'blue',\n    'verbs_ed': 'green',\n    'adverbs_ly': 'orange',\n    'nouns': 'purple',\n    'adjectives': 'red',\n    'pronouns': 'cyan',\n}\n\nfor group_name, group in grouped_words.items():\n    if group:  # Check if the group is not empty\n        sampled_words = random.sample(group, min(num_words_per_group, len(group)))\n        selected_words += sampled_words\n        selected_group_labels += [group_name] * len(sampled_words)  # Label for the group\n\n# Step 2: Extract the embeddings for the selected words\nselected_embeddings = []\nselected_word_labels = []\n\nfor word in selected_words:\n    if word in stoi:  # Ensure the word exists in stoi\n        idx = stoi[word]  # Get the index of the word\n        selected_embeddings.append(embedding_weights[idx])  # Get the embedding\n        selected_word_labels.append(word)  # Save the word for labeling\n\nselected_embeddings = torch.tensor(selected_embeddings)  # Convert to tensor\n\n# Step 3: Reduce dimensionality using t-SNE\ntsne = TSNE(n_components=2, random_state=1)\nembeddings_tsne = tsne.fit_transform(selected_embeddings)\n\n# Step 4: Plot the selected words' embeddings\nplt.figure(figsize=(12, 10))\n\n# Use different colors for each group\nfor group_name, color in colors.items():\n    indices = [i for i, label in enumerate(selected_group_labels) if label == group_name]\n    plt.scatter(embeddings_tsne[indices, 0], embeddings_tsne[indices, 1], \n                alpha=0.6, label=group_name, color=color, s=100)  # Increase marker size\n\n# # Annotate the plot with the corresponding words\n# for i, word in enumerate(selected_word_labels):\n#     plt.annotate(word, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]), \n#                  fontsize=9, alpha=0.75)\n\n# Add legend\nplt.legend(title='Word Groups', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title('t-SNE Visualization of Grouped Word Embeddings', fontsize=16)\nplt.xlabel('t-SNE Component 1', fontsize=14)\nplt.ylabel('t-SNE Component 2', fontsize=14)\nplt.grid(True)  # Optional: Add grid for better readability\nplt.tight_layout()  # Adjust layout\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.442708Z","iopub.status.idle":"2024-10-25T01:09:21.443078Z","shell.execute_reply.started":"2024-10-25T01:09:21.442884Z","shell.execute_reply":"2024-10-25T01:09:21.442919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport random\nimport torch\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Group words based on their suffixes\ndef group_words(itos):\n    groups = {\n        'verbs_ing': [],\n        'verbs_ed': [],\n        'adverbs_ly': [],\n        'nouns': [],\n        'adjectives': [],\n        'pronouns': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'],\n    }\n\n    # Iterate over all words\n    for word in itos.values():\n        # Verbs ending with 'ing'\n        if word.endswith('ing'):\n            groups['verbs_ing'].append(word)\n        # Verbs ending with 'ed'\n        elif word.endswith('ed'):\n            groups['verbs_ed'].append(word)\n        # Adverbs ending with 'ly'\n        elif word.endswith('ly'):\n            groups['adverbs_ly'].append(word)\n        # Nouns: words ending with common noun suffixes\n        elif word.endswith('ness') or word.endswith('ment') or word.endswith('tion'):\n            groups['nouns'].append(word)\n        # Adjectives: words ending with common adjective suffixes\n        elif word.endswith('able') or word.endswith('ous') or word.endswith('ive'):\n            groups['adjectives'].append(word)\n\n    return groups\n\npronouns = ['he', 'she', 'they', 'we', 'i', 'you', 'it', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'their', 'our']\n\ngrouped_words = group_words(itos)\ngrouped_words['pronouns'] = [word for word in pronouns if word in stoi]\nprint(f\"Number of pronouns: {len(grouped_words['pronouns'])}\")\n\n# Select a subset of words for each group (e.g., 50 from each group)\nnum_words_per_group = 50\n\nfor group_name, group in grouped_words.items():\n    selected_words = random.sample(group, min(num_words_per_group, len(group)))\n    \n    # Extract the embeddings for the selected words\n    selected_embeddings = []\n    selected_word_labels = []\n    \n    for word in selected_words:\n        if word in stoi:  # Ensure the word exists in stoi\n            idx = stoi[word]  # Get the index of the word\n            selected_embeddings.append(embedding_weights[idx])  # Get the embedding\n            selected_word_labels.append(word)  # Save the word for labeling\n    \n    selected_embeddings = torch.tensor(selected_embeddings)  # Convert to tensor\n    n_samples = selected_embeddings.shape[0]\n    \n    # Reduce dimensionality using t-SNE\n    perplexity_value = min(30, n_samples - 1)\n    tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=1)\n    embeddings_tsne = tsne.fit_transform(selected_embeddings)\n    \n    # Plot the selected words' embeddings\n    plt.figure(figsize=(10, 8))\n    plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5)\n    \n    # Annotate the plot with the corresponding words\n    for i, word in enumerate(selected_word_labels):\n        plt.annotate(word, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]), fontsize=9, alpha=0.75)\n    \n    plt.title(f't-SNE Visualization of Group: {group_name}')\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n\n    # Set equal scaling\n    plt.axis('equal')  # This will make the axes have the same scale\n    \n    plt.show()\n    \n    # Ask user if they want to visualize the next group\n    user_input = input(f\"Do you want to proceed to the next group? (y/n): \")\n    if user_input.lower() != 'y':\n        break\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T01:09:21.444636Z","iopub.status.idle":"2024-10-25T01:09:21.445012Z","shell.execute_reply.started":"2024-10-25T01:09:21.444825Z","shell.execute_reply":"2024-10-25T01:09:21.444845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ","metadata":{"jp-MarkdownHeadingCollapsed":true}}]}