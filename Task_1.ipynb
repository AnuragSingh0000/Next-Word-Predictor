{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:11.193244Z","iopub.status.busy":"2024-10-22T20:44:11.192648Z","iopub.status.idle":"2024-10-22T20:44:29.279152Z","shell.execute_reply":"2024-10-22T20:44:29.278379Z","shell.execute_reply.started":"2024-10-22T20:44:11.193182Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.functional as F\n","import torch.nn as nn\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import Model, Input, layers, models, optimizers\n","\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","from pprint import pprint\n","import re"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:29.281105Z","iopub.status.busy":"2024-10-22T20:44:29.280601Z","iopub.status.idle":"2024-10-22T20:44:29.287963Z","shell.execute_reply":"2024-10-22T20:44:29.286927Z","shell.execute_reply.started":"2024-10-22T20:44:29.281071Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'2.4.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:29.289808Z","iopub.status.busy":"2024-10-22T20:44:29.289366Z","iopub.status.idle":"2024-10-22T20:44:29.367253Z","shell.execute_reply":"2024-10-22T20:44:29.366335Z","shell.execute_reply.started":"2024-10-22T20:44:29.289760Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:29.371003Z","iopub.status.busy":"2024-10-22T20:44:29.370300Z","iopub.status.idle":"2024-10-22T20:44:29.378505Z","shell.execute_reply":"2024-10-22T20:44:29.377689Z","shell.execute_reply.started":"2024-10-22T20:44:29.370950Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:29.380289Z","iopub.status.busy":"2024-10-22T20:44:29.379688Z","iopub.status.idle":"2024-10-22T20:44:44.994583Z","shell.execute_reply":"2024-10-22T20:44:44.993616Z","shell.execute_reply.started":"2024-10-22T20:44:29.380239Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total no. of para:  12426\n","Total unique words:  17879\n"]}],"source":["from os import linesep\n","import string\n","\n","# Read the file\n","file_path = '/kaggle/input/text-for-next-word-predictor/leo tolstoy - war and peace.txt'\n","\n","# Open and read the contents of the file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","filtered_text = re.sub(r'-', ' ', text)\n","filtered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\n","filtered_text = re.sub(r'\\.{1,}', '', filtered_text)\n","filtered_text = filtered_text.lower()\n","\n","words=[]\n","for (word) in filtered_text.split():\n","    if word not in words:\n","        words.append(word)\n","\n","para=filtered_text.split(\"\\n\\n\")\n","print(\"Total no. of para: \", len(para))\n","print(\"Total unique words: \", len(words))"]},{"cell_type":"code","execution_count":6,"metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2024-10-22T20:44:44.995902Z","iopub.status.busy":"2024-10-22T20:44:44.995623Z","iopub.status.idle":"2024-10-22T20:44:45.007791Z","shell.execute_reply":"2024-10-22T20:44:45.006861Z","shell.execute_reply.started":"2024-10-22T20:44:44.995872Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["17880\n"]}],"source":["stoi={s:i+1 for i,s in enumerate(words)}\n","stoi['.']=0\n","itos={i:s for s,i in stoi.items()}\n","print(len(itos))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:45.009506Z","iopub.status.busy":"2024-10-22T20:44:45.009113Z","iopub.status.idle":"2024-10-22T20:44:48.119625Z","shell.execute_reply":"2024-10-22T20:44:48.118682Z","shell.execute_reply.started":"2024-10-22T20:44:45.009461Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([565951, 5]), torch.Size([565951]), torch.int64, torch.int64)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Hyperparameter\n","block_size=5 # context_length: how many words do we take to predict the next one\n","\n","# X and Y matrices to store the data for training\n","# X stores the half lines\n","# Y stores the next word\n","X,Y=[],[]\n","device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","for p in para:\n","  context=[0]*block_size\n","\n","  for word in p.split():\n","    word=word.rstrip(string.punctuation)\n","    ix=stoi[word]\n","    X.append(context)\n","    Y.append(ix)\n","    # print(' '.join(itos[i] for i in context), '--->', itos[ix])\n","    context = context[1:] + [ix]\n","\n","\n","# Move data to GPU\n","\n","X = torch.tensor(X).to(device)\n","Y = torch.tensor(Y).to(device)\n","\n","\n","X.shape, Y.shape, X.dtype, Y.dtype"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:48.121382Z","iopub.status.busy":"2024-10-22T20:44:48.120968Z","iopub.status.idle":"2024-10-22T20:44:48.156801Z","shell.execute_reply":"2024-10-22T20:44:48.155875Z","shell.execute_reply.started":"2024-10-22T20:44:48.121337Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Embedding(17880, 64)\n"]}],"source":["emb_dim = 64 # Hyperparameter\n","\n","# Embedding layer\n","emb=torch.nn.Embedding(len(stoi),emb_dim).to(device)\n","print(emb)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T20:44:48.158239Z","iopub.status.busy":"2024-10-22T20:44:48.157952Z","iopub.status.idle":"2024-10-22T20:44:48.165246Z","shell.execute_reply":"2024-10-22T20:44:48.164265Z","shell.execute_reply.started":"2024-10-22T20:44:48.158207Z"},"trusted":true},"outputs":[],"source":["class Next_Word_Predictor(nn.Module):\n","  def __init__(self, block_size, vocab_size, emb_dim, hidden_dim):\n","    super().__init__()\n","    # Input size: vocab_size (the total number of characters in the vocabulary).\n","    # Output size: emb_dim (the size of the dense vector representation for each character).\n","    self.emb=nn.Embedding(vocab_size, emb_dim)\n","    # Input size: block_size * emb_dim\n","    # Output size: hidden_dim (the size of the hidden layer).\n","    self.linear1=nn.Linear(block_size*emb_dim, hidden_dim)\n","    # Input size: hidden_dim\n","    # Output size: vocab_size (the total number of words in the vocabulary).\n","    self.linear2=nn.Linear(hidden_dim, vocab_size)\n","\n","  def forward(self, x):\n","    # I/P layer\n","    x = self.emb(x)\n","    x = x.view(x.shape[0], -1)\n","    # Hidden layer\n","    x = self.linear1(x)\n","    x = torch.relu(x)\n","    # Output layer\n","    x = self.linear2(x)\n","    return x"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T21:01:41.058088Z","iopub.status.busy":"2024-10-22T21:01:41.057707Z","iopub.status.idle":"2024-10-22T21:01:41.066493Z","shell.execute_reply":"2024-10-22T21:01:41.065519Z","shell.execute_reply.started":"2024-10-22T21:01:41.058050Z"},"trusted":true},"outputs":[],"source":["# Generate names from untrained model\n","\n","\n","def generate_next_words(model, itos, stoi, content, block_size, k=10, max_len=10):\n","    context = content.lower()\n","    context = re.sub('[^a-zA-Z0-9 \\.]', '', context)\n","    context = [stoi[word.strip(string.punctuation)] for word in context.split()]\n","\n","    if len(context) <= block_size:\n","        context = [0] * (block_size - len(context)) + context\n","    elif len(context) > block_size:\n","        context = context[-block_size:]\n","\n","    for i in range(k):\n","        x = torch.tensor(context).view(1, -1).to(device)\n","        y_pred = model(x)\n","        logits = y_pred\n","        \n","        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n","        word = itos[ix]\n","        content += \" \" + word\n","        context = context [1:] + [ix]\n","        \n","    return content\n"]},{"cell_type":"code","execution_count":21,"metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2024-10-22T21:02:43.405897Z","iopub.status.busy":"2024-10-22T21:02:43.405159Z","iopub.status.idle":"2024-10-22T21:02:53.996811Z","shell.execute_reply":"2024-10-22T21:02:53.995404Z","shell.execute_reply.started":"2024-10-22T21:02:43.405856Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0 6.391009330749512\n","1 5.832068920135498\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","\n","# Train the model\n","model=Next_Word_Predictor(block_size, len(stoi), emb_dim, 100).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n","\n","# Mini-batch training\n","batch_size = 1024\n","print_every = 1\n","elapsed_time = []\n","\n","for epoch in range(2000):\n","    start_time = time.time()\n","    for i in range(0, X.shape[0], batch_size):\n","        x = X[i:i+batch_size]\n","        y = Y[i:i+batch_size]\n","        y_pred = model(x)\n","        loss = loss_fn(y_pred, y)\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","    end_time = time.time()\n","    elapsed_time.append(end_time - start_time)\n","    if epoch % print_every == 0:\n","        print(epoch, loss.item())"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T21:01:45.282042Z","iopub.status.busy":"2024-10-22T21:01:45.281662Z","iopub.status.idle":"2024-10-22T21:01:49.646856Z","shell.execute_reply":"2024-10-22T21:01:49.645790Z","shell.execute_reply.started":"2024-10-22T21:01:45.282004Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter some context:  letters of inquiry and notices from the court arrived\n","Enter no. of words to be generated:  10\n"]},{"name":"stdout","output_type":"stream","text":[" letters of inquiry and notices from the court arrived and from the screaming political left excitement to the company\n","\n","letters of inquiry and notices from the court arrived back the enemy was a long ago you trouble was\n","\n","letters of inquiry and notices from the court arrived at last listen nurse dont think of morning his cheeks\n","\n","letters of inquiry and notices from the court arrived with front he firmly fixed it i shall be helped\n","\n","letters of inquiry and notices from the court arrived and learning the thousands of times stood at the entrenchment\n","\n","letters of inquiry and notices from the court arrived angrily the blonde officers on him went out to their\n","\n","letters of inquiry and notices from the court arrived with his leader house owned he and knows what since\n","\n","letters of inquiry and notices from the court arrived in the ground a healthy and thick coat seated ran\n","\n","letters of inquiry and notices from the court arrived as if they drank a smiling in russian soldier and\n","\n","letters of inquiry and notices from the court arrived with the visitors had been heard of the outspread quarters\n","\n","\n"]}],"source":["# Generate names from trained model\n","\n","para=\" \"\n","content=input(\"Enter some context: \")\n","k=int(input(\"Enter no. of words to be generated: \"))\n","for i in range(10):\n","    para+=generate_next_words(model, itos, stoi, content, block_size, k)\n","    para+=\"\\n\\n\"\n","print(para)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving the model\n","\n","torch.save(model, 'next_word_predictor_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-22T20:47:03.375490Z","iopub.status.idle":"2024-10-22T20:47:03.375951Z","shell.execute_reply":"2024-10-22T20:47:03.375740Z","shell.execute_reply.started":"2024-10-22T20:47:03.375717Z"},"trusted":true},"outputs":[],"source":["embedding_weights = model.emb.weights[0].numpy()\n","print(embedding_weights.shape)\n","# Reduce dimensionality using t-SNE\n","tsne = TSNE(n_components=2, random_state=42)\n","embeddings_tsne = tsne.fit_transform(embedding_weights)\n","\n","# Visualize embeddings\n","plt.figure(figsize=(10, 8))\n","plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5)\n","plt.title('t-SNE Visualization of Embeddings')  \n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5926950,"sourceId":9694017,"sourceType":"datasetVersion"},{"datasetId":5928262,"sourceId":9695710,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
