{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jm_YZZfiUadv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from pprint import pprint\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jBAPgv-fVVxL",
        "outputId": "fb7807b3-97a1-4a43-9a7e-396d9c003ec7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import linesep\n",
        "text=\"\"\"\"Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "if you still try to defend the infamies and horrors perpetrated by that\n",
        "Antichrist--I really believe he is Antichrist--I will have nothing more\n",
        "to do with you and you are no longer my friend, no longer my 'faithful\n",
        "slave,' as you call yourself! But how do you do? I see I have frightened\n",
        "you--sit down and tell me all the news.\"\n",
        "\n",
        "It was in July, 1805, and the speaker was the well-known Anna Pavlovna\n",
        "Scherer, maid of honor and favorite of the Empress Marya Fedorovna. With\n",
        "these words she greeted Prince Vasili Kuragin, a man of high rank and\n",
        "importance, who was the first to arrive at her reception. Anna Pavlovna\n",
        "had had a cough for some days. She was, as she said, suffering from la\n",
        "grippe; grippe being then a new word in St. Petersburg, used only by the\n",
        "elite.\n",
        "\n",
        "All her invitations without exception, written in French, and delivered\n",
        "by a scarlet-liveried footman that morning, ran as follows:\n",
        "\n",
        "\"If you have nothing better to do, Count (or Prince), and if the\n",
        "prospect of spending an evening with a poor invalid is not too terrible,\n",
        "I shall be very charmed to see you tonight between 7 and 10--Annette\n",
        "Scherer.\"\n",
        "\n",
        "\"Heavens! what a virulent attack!\" replied the prince, not in the least\n",
        "disconcerted by this reception. He had just entered, wearing an\n",
        "embroidered court uniform, knee breeches, and shoes, and had stars on\n",
        "his breast and a serene expression on his flat face. He spoke in that\n",
        "refined French in which our grandfathers not only spoke but thought, and\n",
        "with the gentle, patronizing intonation natural to a man of importance\n",
        "who had grown old in society and at court. He went up to Anna Pavlovna,\n",
        "kissed her hand, presenting to her his bald, scented, and shining head,\n",
        "and complacently seated himself on the sofa.\n",
        "\n",
        "\"First of all, dear friend, tell me how you are. Set your friend's mind\n",
        "at rest,\" said he without altering his tone, beneath the politeness and\n",
        "affected sympathy of which indifference and even irony could be\n",
        "discerned.\"\"\"\n",
        "\n",
        "filtered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', text)\n",
        "\n",
        "print(filtered_text)\n",
        "words=[]\n",
        "for (word) in filtered_text.split():\n",
        "  if word.rstrip(\".\") not in words:\n",
        "    words.append(word.rstrip(\".\"))\n",
        "  if (word[-1]=='.'):\n",
        "    words.append('.')\n",
        "\n",
        "para=filtered_text.split(\"\\n\\n\")\n",
        "print(len(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvlKZ7kCVprw",
        "outputId": "3c988022-8b23-4765-fcab-4fe9390e3818",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well Prince so Genoa and Lucca are now just family estates of the\n",
            "Buonapartes. But I warn you if you dont tell me that this means war\n",
            "if you still try to defend the infamies and horrors perpetrated by that\n",
            "AntichristI really believe he is AntichristI will have nothing more\n",
            "to do with you and you are no longer my friend no longer my faithful\n",
            "slave as you call yourself But how do you do I see I have frightened\n",
            "yousit down and tell me all the news.\n",
            "\n",
            "It was in July 1805 and the speaker was the wellknown Anna Pavlovna\n",
            "Scherer maid of honor and favorite of the Empress Marya Fedorovna. With\n",
            "these words she greeted Prince Vasili Kuragin a man of high rank and\n",
            "importance who was the first to arrive at her reception. Anna Pavlovna\n",
            "had had a cough for some days. She was as she said suffering from la\n",
            "grippe grippe being then a new word in St. Petersburg used only by the\n",
            "elite.\n",
            "\n",
            "All her invitations without exception written in French and delivered\n",
            "by a scarletliveried footman that morning ran as follows\n",
            "\n",
            "If you have nothing better to do Count or Prince and if the\n",
            "prospect of spending an evening with a poor invalid is not too terrible\n",
            "I shall be very charmed to see you tonight between 7 and 10Annette\n",
            "Scherer.\n",
            "\n",
            "Heavens what a virulent attack replied the prince not in the least\n",
            "disconcerted by this reception. He had just entered wearing an\n",
            "embroidered court uniform knee breeches and shoes and had stars on\n",
            "his breast and a serene expression on his flat face. He spoke in that\n",
            "refined French in which our grandfathers not only spoke but thought and\n",
            "with the gentle patronizing intonation natural to a man of importance\n",
            "who had grown old in society and at court. He went up to Anna Pavlovna\n",
            "kissed her hand presenting to her his bald scented and shining head\n",
            "and complacently seated himself on the sofa.\n",
            "\n",
            "First of all dear friend tell me how you are. Set your friends mind\n",
            "at rest said he without altering his tone beneath the politeness and\n",
            "affected sympathy of which indifference and even irony could be\n",
            "discerned.\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi={s:i+1 for i,s in enumerate(words)}\n",
        "stoi['.']=0\n",
        "itos={i:s for s,i in stoi.items()}\n",
        "pprint(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdqriL5zXuOf",
        "outputId": "df145ff3-fc73-4331-9d10-1bce9b40de78",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '.',\n",
            " 1: 'Well',\n",
            " 2: 'Prince',\n",
            " 3: 'so',\n",
            " 4: 'Genoa',\n",
            " 5: 'and',\n",
            " 6: 'Lucca',\n",
            " 7: 'are',\n",
            " 8: 'now',\n",
            " 9: 'just',\n",
            " 10: 'family',\n",
            " 11: 'estates',\n",
            " 12: 'of',\n",
            " 13: 'the',\n",
            " 14: 'Buonapartes',\n",
            " 16: 'But',\n",
            " 17: 'I',\n",
            " 18: 'warn',\n",
            " 19: 'you',\n",
            " 20: 'if',\n",
            " 21: 'dont',\n",
            " 22: 'tell',\n",
            " 23: 'me',\n",
            " 24: 'that',\n",
            " 25: 'this',\n",
            " 26: 'means',\n",
            " 27: 'war',\n",
            " 28: 'still',\n",
            " 29: 'try',\n",
            " 30: 'to',\n",
            " 31: 'defend',\n",
            " 32: 'infamies',\n",
            " 33: 'horrors',\n",
            " 34: 'perpetrated',\n",
            " 35: 'by',\n",
            " 36: 'AntichristI',\n",
            " 37: 'really',\n",
            " 38: 'believe',\n",
            " 39: 'he',\n",
            " 40: 'is',\n",
            " 41: 'will',\n",
            " 42: 'have',\n",
            " 43: 'nothing',\n",
            " 44: 'more',\n",
            " 45: 'do',\n",
            " 46: 'with',\n",
            " 47: 'no',\n",
            " 48: 'longer',\n",
            " 49: 'my',\n",
            " 50: 'friend',\n",
            " 51: 'faithful',\n",
            " 52: 'slave',\n",
            " 53: 'as',\n",
            " 54: 'call',\n",
            " 55: 'yourself',\n",
            " 56: 'how',\n",
            " 57: 'see',\n",
            " 58: 'frightened',\n",
            " 59: 'yousit',\n",
            " 60: 'down',\n",
            " 61: 'all',\n",
            " 62: 'news',\n",
            " 64: 'It',\n",
            " 65: 'was',\n",
            " 66: 'in',\n",
            " 67: 'July',\n",
            " 68: '1805',\n",
            " 69: 'speaker',\n",
            " 70: 'wellknown',\n",
            " 71: 'Anna',\n",
            " 72: 'Pavlovna',\n",
            " 73: 'Scherer',\n",
            " 74: 'maid',\n",
            " 75: 'honor',\n",
            " 76: 'favorite',\n",
            " 77: 'Empress',\n",
            " 78: 'Marya',\n",
            " 79: 'Fedorovna',\n",
            " 81: 'With',\n",
            " 82: 'these',\n",
            " 83: 'words',\n",
            " 84: 'she',\n",
            " 85: 'greeted',\n",
            " 86: 'Vasili',\n",
            " 87: 'Kuragin',\n",
            " 88: 'a',\n",
            " 89: 'man',\n",
            " 90: 'high',\n",
            " 91: 'rank',\n",
            " 92: 'importance',\n",
            " 93: 'who',\n",
            " 94: 'first',\n",
            " 95: 'arrive',\n",
            " 96: 'at',\n",
            " 97: 'her',\n",
            " 98: 'reception',\n",
            " 100: 'had',\n",
            " 101: 'cough',\n",
            " 102: 'for',\n",
            " 103: 'some',\n",
            " 104: 'days',\n",
            " 106: 'She',\n",
            " 107: 'said',\n",
            " 108: 'suffering',\n",
            " 109: 'from',\n",
            " 110: 'la',\n",
            " 111: 'grippe',\n",
            " 112: 'being',\n",
            " 113: 'then',\n",
            " 114: 'new',\n",
            " 115: 'word',\n",
            " 116: 'St',\n",
            " 118: 'Petersburg',\n",
            " 119: 'used',\n",
            " 120: 'only',\n",
            " 121: 'elite',\n",
            " 123: 'All',\n",
            " 124: 'invitations',\n",
            " 125: 'without',\n",
            " 126: 'exception',\n",
            " 127: 'written',\n",
            " 128: 'French',\n",
            " 129: 'delivered',\n",
            " 130: 'scarletliveried',\n",
            " 131: 'footman',\n",
            " 132: 'morning',\n",
            " 133: 'ran',\n",
            " 134: 'follows',\n",
            " 135: 'If',\n",
            " 136: 'better',\n",
            " 137: 'Count',\n",
            " 138: 'or',\n",
            " 139: 'prospect',\n",
            " 140: 'spending',\n",
            " 141: 'an',\n",
            " 142: 'evening',\n",
            " 143: 'poor',\n",
            " 144: 'invalid',\n",
            " 145: 'not',\n",
            " 146: 'too',\n",
            " 147: 'terrible',\n",
            " 148: 'shall',\n",
            " 149: 'be',\n",
            " 150: 'very',\n",
            " 151: 'charmed',\n",
            " 152: 'tonight',\n",
            " 153: 'between',\n",
            " 154: '7',\n",
            " 155: '10Annette',\n",
            " 157: 'Heavens',\n",
            " 158: 'what',\n",
            " 159: 'virulent',\n",
            " 160: 'attack',\n",
            " 161: 'replied',\n",
            " 162: 'prince',\n",
            " 163: 'least',\n",
            " 164: 'disconcerted',\n",
            " 166: 'He',\n",
            " 167: 'entered',\n",
            " 168: 'wearing',\n",
            " 169: 'embroidered',\n",
            " 170: 'court',\n",
            " 171: 'uniform',\n",
            " 172: 'knee',\n",
            " 173: 'breeches',\n",
            " 174: 'shoes',\n",
            " 175: 'stars',\n",
            " 176: 'on',\n",
            " 177: 'his',\n",
            " 178: 'breast',\n",
            " 179: 'serene',\n",
            " 180: 'expression',\n",
            " 181: 'flat',\n",
            " 182: 'face',\n",
            " 184: 'spoke',\n",
            " 185: 'refined',\n",
            " 186: 'which',\n",
            " 187: 'our',\n",
            " 188: 'grandfathers',\n",
            " 189: 'but',\n",
            " 190: 'thought',\n",
            " 191: 'gentle',\n",
            " 192: 'patronizing',\n",
            " 193: 'intonation',\n",
            " 194: 'natural',\n",
            " 195: 'grown',\n",
            " 196: 'old',\n",
            " 197: 'society',\n",
            " 199: 'went',\n",
            " 200: 'up',\n",
            " 201: 'kissed',\n",
            " 202: 'hand',\n",
            " 203: 'presenting',\n",
            " 204: 'bald',\n",
            " 205: 'scented',\n",
            " 206: 'shining',\n",
            " 207: 'head',\n",
            " 208: 'complacently',\n",
            " 209: 'seated',\n",
            " 210: 'himself',\n",
            " 211: 'sofa',\n",
            " 213: 'First',\n",
            " 214: 'dear',\n",
            " 216: 'Set',\n",
            " 217: 'your',\n",
            " 218: 'friends',\n",
            " 219: 'mind',\n",
            " 220: 'rest',\n",
            " 221: 'altering',\n",
            " 222: 'tone',\n",
            " 223: 'beneath',\n",
            " 224: 'politeness',\n",
            " 225: 'affected',\n",
            " 226: 'sympathy',\n",
            " 227: 'indifference',\n",
            " 228: 'even',\n",
            " 229: 'irony',\n",
            " 230: 'could',\n",
            " 231: 'discerned'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "block_size=5 # context_length: how many words do we take to predict the next one\n",
        "\n",
        "# X and Y matrices to store the data for training\n",
        "# X stores the half lines\n",
        "# Y stores the next word\n",
        "X,Y=[],[]\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for p in para:\n",
        "  context=[0]*block_size\n",
        "\n",
        "  for word in p.split():\n",
        "    if (word[-1]=='.'):\n",
        "      word=word[:-1]\n",
        "      ix=stoi[word]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      print(' '.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "      word='.'\n",
        "      ix=stoi[word]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      print(' '.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "    else:\n",
        "      ix=stoi[word]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      print(' '.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "\n",
        "# Move data to GPU\n",
        "\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n"
      ],
      "metadata": {
        "id": "ueDf-4NIfr5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37998208-3467-4a17-ee3c-b8f2c19f5d37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". . . . . ---> Well\n",
            ". . . . Well ---> Prince\n",
            ". . . Well Prince ---> so\n",
            ". . Well Prince so ---> Genoa\n",
            ". Well Prince so Genoa ---> and\n",
            "Well Prince so Genoa and ---> Lucca\n",
            "Prince so Genoa and Lucca ---> are\n",
            "so Genoa and Lucca are ---> now\n",
            "Genoa and Lucca are now ---> just\n",
            "and Lucca are now just ---> family\n",
            "Lucca are now just family ---> estates\n",
            "are now just family estates ---> of\n",
            "now just family estates of ---> the\n",
            "just family estates of the ---> Buonapartes\n",
            "family estates of the Buonapartes ---> .\n",
            "estates of the Buonapartes . ---> But\n",
            "of the Buonapartes . But ---> I\n",
            "the Buonapartes . But I ---> warn\n",
            "Buonapartes . But I warn ---> you\n",
            ". But I warn you ---> if\n",
            "But I warn you if ---> you\n",
            "I warn you if you ---> dont\n",
            "warn you if you dont ---> tell\n",
            "you if you dont tell ---> me\n",
            "if you dont tell me ---> that\n",
            "you dont tell me that ---> this\n",
            "dont tell me that this ---> means\n",
            "tell me that this means ---> war\n",
            "me that this means war ---> if\n",
            "that this means war if ---> you\n",
            "this means war if you ---> still\n",
            "means war if you still ---> try\n",
            "war if you still try ---> to\n",
            "if you still try to ---> defend\n",
            "you still try to defend ---> the\n",
            "still try to defend the ---> infamies\n",
            "try to defend the infamies ---> and\n",
            "to defend the infamies and ---> horrors\n",
            "defend the infamies and horrors ---> perpetrated\n",
            "the infamies and horrors perpetrated ---> by\n",
            "infamies and horrors perpetrated by ---> that\n",
            "and horrors perpetrated by that ---> AntichristI\n",
            "horrors perpetrated by that AntichristI ---> really\n",
            "perpetrated by that AntichristI really ---> believe\n",
            "by that AntichristI really believe ---> he\n",
            "that AntichristI really believe he ---> is\n",
            "AntichristI really believe he is ---> AntichristI\n",
            "really believe he is AntichristI ---> will\n",
            "believe he is AntichristI will ---> have\n",
            "he is AntichristI will have ---> nothing\n",
            "is AntichristI will have nothing ---> more\n",
            "AntichristI will have nothing more ---> to\n",
            "will have nothing more to ---> do\n",
            "have nothing more to do ---> with\n",
            "nothing more to do with ---> you\n",
            "more to do with you ---> and\n",
            "to do with you and ---> you\n",
            "do with you and you ---> are\n",
            "with you and you are ---> no\n",
            "you and you are no ---> longer\n",
            "and you are no longer ---> my\n",
            "you are no longer my ---> friend\n",
            "are no longer my friend ---> no\n",
            "no longer my friend no ---> longer\n",
            "longer my friend no longer ---> my\n",
            "my friend no longer my ---> faithful\n",
            "friend no longer my faithful ---> slave\n",
            "no longer my faithful slave ---> as\n",
            "longer my faithful slave as ---> you\n",
            "my faithful slave as you ---> call\n",
            "faithful slave as you call ---> yourself\n",
            "slave as you call yourself ---> But\n",
            "as you call yourself But ---> how\n",
            "you call yourself But how ---> do\n",
            "call yourself But how do ---> you\n",
            "yourself But how do you ---> do\n",
            "But how do you do ---> I\n",
            "how do you do I ---> see\n",
            "do you do I see ---> I\n",
            "you do I see I ---> have\n",
            "do I see I have ---> frightened\n",
            "I see I have frightened ---> yousit\n",
            "see I have frightened yousit ---> down\n",
            "I have frightened yousit down ---> and\n",
            "have frightened yousit down and ---> tell\n",
            "frightened yousit down and tell ---> me\n",
            "yousit down and tell me ---> all\n",
            "down and tell me all ---> the\n",
            "and tell me all the ---> news\n",
            "tell me all the news ---> .\n",
            ". . . . . ---> It\n",
            ". . . . It ---> was\n",
            ". . . It was ---> in\n",
            ". . It was in ---> July\n",
            ". It was in July ---> 1805\n",
            "It was in July 1805 ---> and\n",
            "was in July 1805 and ---> the\n",
            "in July 1805 and the ---> speaker\n",
            "July 1805 and the speaker ---> was\n",
            "1805 and the speaker was ---> the\n",
            "and the speaker was the ---> wellknown\n",
            "the speaker was the wellknown ---> Anna\n",
            "speaker was the wellknown Anna ---> Pavlovna\n",
            "was the wellknown Anna Pavlovna ---> Scherer\n",
            "the wellknown Anna Pavlovna Scherer ---> maid\n",
            "wellknown Anna Pavlovna Scherer maid ---> of\n",
            "Anna Pavlovna Scherer maid of ---> honor\n",
            "Pavlovna Scherer maid of honor ---> and\n",
            "Scherer maid of honor and ---> favorite\n",
            "maid of honor and favorite ---> of\n",
            "of honor and favorite of ---> the\n",
            "honor and favorite of the ---> Empress\n",
            "and favorite of the Empress ---> Marya\n",
            "favorite of the Empress Marya ---> Fedorovna\n",
            "of the Empress Marya Fedorovna ---> .\n",
            "the Empress Marya Fedorovna . ---> With\n",
            "Empress Marya Fedorovna . With ---> these\n",
            "Marya Fedorovna . With these ---> words\n",
            "Fedorovna . With these words ---> she\n",
            ". With these words she ---> greeted\n",
            "With these words she greeted ---> Prince\n",
            "these words she greeted Prince ---> Vasili\n",
            "words she greeted Prince Vasili ---> Kuragin\n",
            "she greeted Prince Vasili Kuragin ---> a\n",
            "greeted Prince Vasili Kuragin a ---> man\n",
            "Prince Vasili Kuragin a man ---> of\n",
            "Vasili Kuragin a man of ---> high\n",
            "Kuragin a man of high ---> rank\n",
            "a man of high rank ---> and\n",
            "man of high rank and ---> importance\n",
            "of high rank and importance ---> who\n",
            "high rank and importance who ---> was\n",
            "rank and importance who was ---> the\n",
            "and importance who was the ---> first\n",
            "importance who was the first ---> to\n",
            "who was the first to ---> arrive\n",
            "was the first to arrive ---> at\n",
            "the first to arrive at ---> her\n",
            "first to arrive at her ---> reception\n",
            "to arrive at her reception ---> .\n",
            "arrive at her reception . ---> Anna\n",
            "at her reception . Anna ---> Pavlovna\n",
            "her reception . Anna Pavlovna ---> had\n",
            "reception . Anna Pavlovna had ---> had\n",
            ". Anna Pavlovna had had ---> a\n",
            "Anna Pavlovna had had a ---> cough\n",
            "Pavlovna had had a cough ---> for\n",
            "had had a cough for ---> some\n",
            "had a cough for some ---> days\n",
            "a cough for some days ---> .\n",
            "cough for some days . ---> She\n",
            "for some days . She ---> was\n",
            "some days . She was ---> as\n",
            "days . She was as ---> she\n",
            ". She was as she ---> said\n",
            "She was as she said ---> suffering\n",
            "was as she said suffering ---> from\n",
            "as she said suffering from ---> la\n",
            "she said suffering from la ---> grippe\n",
            "said suffering from la grippe ---> grippe\n",
            "suffering from la grippe grippe ---> being\n",
            "from la grippe grippe being ---> then\n",
            "la grippe grippe being then ---> a\n",
            "grippe grippe being then a ---> new\n",
            "grippe being then a new ---> word\n",
            "being then a new word ---> in\n",
            "then a new word in ---> St\n",
            "a new word in St ---> .\n",
            "new word in St . ---> Petersburg\n",
            "word in St . Petersburg ---> used\n",
            "in St . Petersburg used ---> only\n",
            "St . Petersburg used only ---> by\n",
            ". Petersburg used only by ---> the\n",
            "Petersburg used only by the ---> elite\n",
            "used only by the elite ---> .\n",
            ". . . . . ---> All\n",
            ". . . . All ---> her\n",
            ". . . All her ---> invitations\n",
            ". . All her invitations ---> without\n",
            ". All her invitations without ---> exception\n",
            "All her invitations without exception ---> written\n",
            "her invitations without exception written ---> in\n",
            "invitations without exception written in ---> French\n",
            "without exception written in French ---> and\n",
            "exception written in French and ---> delivered\n",
            "written in French and delivered ---> by\n",
            "in French and delivered by ---> a\n",
            "French and delivered by a ---> scarletliveried\n",
            "and delivered by a scarletliveried ---> footman\n",
            "delivered by a scarletliveried footman ---> that\n",
            "by a scarletliveried footman that ---> morning\n",
            "a scarletliveried footman that morning ---> ran\n",
            "scarletliveried footman that morning ran ---> as\n",
            "footman that morning ran as ---> follows\n",
            ". . . . . ---> If\n",
            ". . . . If ---> you\n",
            ". . . If you ---> have\n",
            ". . If you have ---> nothing\n",
            ". If you have nothing ---> better\n",
            "If you have nothing better ---> to\n",
            "you have nothing better to ---> do\n",
            "have nothing better to do ---> Count\n",
            "nothing better to do Count ---> or\n",
            "better to do Count or ---> Prince\n",
            "to do Count or Prince ---> and\n",
            "do Count or Prince and ---> if\n",
            "Count or Prince and if ---> the\n",
            "or Prince and if the ---> prospect\n",
            "Prince and if the prospect ---> of\n",
            "and if the prospect of ---> spending\n",
            "if the prospect of spending ---> an\n",
            "the prospect of spending an ---> evening\n",
            "prospect of spending an evening ---> with\n",
            "of spending an evening with ---> a\n",
            "spending an evening with a ---> poor\n",
            "an evening with a poor ---> invalid\n",
            "evening with a poor invalid ---> is\n",
            "with a poor invalid is ---> not\n",
            "a poor invalid is not ---> too\n",
            "poor invalid is not too ---> terrible\n",
            "invalid is not too terrible ---> I\n",
            "is not too terrible I ---> shall\n",
            "not too terrible I shall ---> be\n",
            "too terrible I shall be ---> very\n",
            "terrible I shall be very ---> charmed\n",
            "I shall be very charmed ---> to\n",
            "shall be very charmed to ---> see\n",
            "be very charmed to see ---> you\n",
            "very charmed to see you ---> tonight\n",
            "charmed to see you tonight ---> between\n",
            "to see you tonight between ---> 7\n",
            "see you tonight between 7 ---> and\n",
            "you tonight between 7 and ---> 10Annette\n",
            "tonight between 7 and 10Annette ---> Scherer\n",
            "between 7 and 10Annette Scherer ---> .\n",
            ". . . . . ---> Heavens\n",
            ". . . . Heavens ---> what\n",
            ". . . Heavens what ---> a\n",
            ". . Heavens what a ---> virulent\n",
            ". Heavens what a virulent ---> attack\n",
            "Heavens what a virulent attack ---> replied\n",
            "what a virulent attack replied ---> the\n",
            "a virulent attack replied the ---> prince\n",
            "virulent attack replied the prince ---> not\n",
            "attack replied the prince not ---> in\n",
            "replied the prince not in ---> the\n",
            "the prince not in the ---> least\n",
            "prince not in the least ---> disconcerted\n",
            "not in the least disconcerted ---> by\n",
            "in the least disconcerted by ---> this\n",
            "the least disconcerted by this ---> reception\n",
            "least disconcerted by this reception ---> .\n",
            "disconcerted by this reception . ---> He\n",
            "by this reception . He ---> had\n",
            "this reception . He had ---> just\n",
            "reception . He had just ---> entered\n",
            ". He had just entered ---> wearing\n",
            "He had just entered wearing ---> an\n",
            "had just entered wearing an ---> embroidered\n",
            "just entered wearing an embroidered ---> court\n",
            "entered wearing an embroidered court ---> uniform\n",
            "wearing an embroidered court uniform ---> knee\n",
            "an embroidered court uniform knee ---> breeches\n",
            "embroidered court uniform knee breeches ---> and\n",
            "court uniform knee breeches and ---> shoes\n",
            "uniform knee breeches and shoes ---> and\n",
            "knee breeches and shoes and ---> had\n",
            "breeches and shoes and had ---> stars\n",
            "and shoes and had stars ---> on\n",
            "shoes and had stars on ---> his\n",
            "and had stars on his ---> breast\n",
            "had stars on his breast ---> and\n",
            "stars on his breast and ---> a\n",
            "on his breast and a ---> serene\n",
            "his breast and a serene ---> expression\n",
            "breast and a serene expression ---> on\n",
            "and a serene expression on ---> his\n",
            "a serene expression on his ---> flat\n",
            "serene expression on his flat ---> face\n",
            "expression on his flat face ---> .\n",
            "on his flat face . ---> He\n",
            "his flat face . He ---> spoke\n",
            "flat face . He spoke ---> in\n",
            "face . He spoke in ---> that\n",
            ". He spoke in that ---> refined\n",
            "He spoke in that refined ---> French\n",
            "spoke in that refined French ---> in\n",
            "in that refined French in ---> which\n",
            "that refined French in which ---> our\n",
            "refined French in which our ---> grandfathers\n",
            "French in which our grandfathers ---> not\n",
            "in which our grandfathers not ---> only\n",
            "which our grandfathers not only ---> spoke\n",
            "our grandfathers not only spoke ---> but\n",
            "grandfathers not only spoke but ---> thought\n",
            "not only spoke but thought ---> and\n",
            "only spoke but thought and ---> with\n",
            "spoke but thought and with ---> the\n",
            "but thought and with the ---> gentle\n",
            "thought and with the gentle ---> patronizing\n",
            "and with the gentle patronizing ---> intonation\n",
            "with the gentle patronizing intonation ---> natural\n",
            "the gentle patronizing intonation natural ---> to\n",
            "gentle patronizing intonation natural to ---> a\n",
            "patronizing intonation natural to a ---> man\n",
            "intonation natural to a man ---> of\n",
            "natural to a man of ---> importance\n",
            "to a man of importance ---> who\n",
            "a man of importance who ---> had\n",
            "man of importance who had ---> grown\n",
            "of importance who had grown ---> old\n",
            "importance who had grown old ---> in\n",
            "who had grown old in ---> society\n",
            "had grown old in society ---> and\n",
            "grown old in society and ---> at\n",
            "old in society and at ---> court\n",
            "in society and at court ---> .\n",
            "society and at court . ---> He\n",
            "and at court . He ---> went\n",
            "at court . He went ---> up\n",
            "court . He went up ---> to\n",
            ". He went up to ---> Anna\n",
            "He went up to Anna ---> Pavlovna\n",
            "went up to Anna Pavlovna ---> kissed\n",
            "up to Anna Pavlovna kissed ---> her\n",
            "to Anna Pavlovna kissed her ---> hand\n",
            "Anna Pavlovna kissed her hand ---> presenting\n",
            "Pavlovna kissed her hand presenting ---> to\n",
            "kissed her hand presenting to ---> her\n",
            "her hand presenting to her ---> his\n",
            "hand presenting to her his ---> bald\n",
            "presenting to her his bald ---> scented\n",
            "to her his bald scented ---> and\n",
            "her his bald scented and ---> shining\n",
            "his bald scented and shining ---> head\n",
            "bald scented and shining head ---> and\n",
            "scented and shining head and ---> complacently\n",
            "and shining head and complacently ---> seated\n",
            "shining head and complacently seated ---> himself\n",
            "head and complacently seated himself ---> on\n",
            "and complacently seated himself on ---> the\n",
            "complacently seated himself on the ---> sofa\n",
            "seated himself on the sofa ---> .\n",
            ". . . . . ---> First\n",
            ". . . . First ---> of\n",
            ". . . First of ---> all\n",
            ". . First of all ---> dear\n",
            ". First of all dear ---> friend\n",
            "First of all dear friend ---> tell\n",
            "of all dear friend tell ---> me\n",
            "all dear friend tell me ---> how\n",
            "dear friend tell me how ---> you\n",
            "friend tell me how you ---> are\n",
            "tell me how you are ---> .\n",
            "me how you are . ---> Set\n",
            "how you are . Set ---> your\n",
            "you are . Set your ---> friends\n",
            "are . Set your friends ---> mind\n",
            ". Set your friends mind ---> at\n",
            "Set your friends mind at ---> rest\n",
            "your friends mind at rest ---> said\n",
            "friends mind at rest said ---> he\n",
            "mind at rest said he ---> without\n",
            "at rest said he without ---> altering\n",
            "rest said he without altering ---> his\n",
            "said he without altering his ---> tone\n",
            "he without altering his tone ---> beneath\n",
            "without altering his tone beneath ---> the\n",
            "altering his tone beneath the ---> politeness\n",
            "his tone beneath the politeness ---> and\n",
            "tone beneath the politeness and ---> affected\n",
            "beneath the politeness and affected ---> sympathy\n",
            "the politeness and affected sympathy ---> of\n",
            "politeness and affected sympathy of ---> which\n",
            "and affected sympathy of which ---> indifference\n",
            "affected sympathy of which indifference ---> and\n",
            "sympathy of which indifference and ---> even\n",
            "of which indifference and even ---> irony\n",
            "which indifference and even irony ---> could\n",
            "indifference and even irony could ---> be\n",
            "and even irony could be ---> discerned\n",
            "even irony could be discerned ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape, X.dtype, Y.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV5EEzHXEkbn",
        "outputId": "0f8ee11b-6f58-4d65-cd5c-d607609a8e64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([382, 5]), torch.Size([382]), torch.int64, torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer for the context\n",
        "\n",
        "emb_dim=2 # Hyperparameter\n",
        "emb=torch.nn.Embedding(len(stoi),emb_dim).to(device)\n",
        "print(emb)\n",
        "\n",
        "# The emb contains weights which are vectors for each of the alphabet\n",
        "print(emb.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyxJMAmLEqZZ",
        "outputId": "a6ea84b7-0cc4-4a79-a45d-fca0371c5cea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(219, 2)\n",
            "Parameter containing:\n",
            "tensor([[-0.6157, -0.1510],\n",
            "        [ 0.1673,  1.7370],\n",
            "        [ 1.1785,  0.3576],\n",
            "        [-1.4603,  0.8752],\n",
            "        [ 0.1656, -0.5529],\n",
            "        [-0.4427,  1.7765],\n",
            "        [ 1.0017,  2.5986],\n",
            "        [-0.2841, -0.2656],\n",
            "        [-0.5087,  1.1264],\n",
            "        [-1.3530,  1.4586],\n",
            "        [-0.6087,  0.2005],\n",
            "        [-0.6741,  0.4103],\n",
            "        [-0.3044, -0.0221],\n",
            "        [ 0.4608, -0.9895],\n",
            "        [ 1.2627,  1.1438],\n",
            "        [-0.8116,  0.1054],\n",
            "        [ 0.2663, -0.0907],\n",
            "        [-0.0184, -0.1006],\n",
            "        [ 2.2148,  0.9812],\n",
            "        [ 0.8752,  0.3022],\n",
            "        [ 0.5831,  0.1853],\n",
            "        [-1.7159, -0.5469],\n",
            "        [ 1.3902,  0.6697],\n",
            "        [-2.1719,  1.2281],\n",
            "        [ 0.1509, -0.2895],\n",
            "        [-1.6336, -0.9729],\n",
            "        [-0.0494, -1.1254],\n",
            "        [-0.5813, -0.9846],\n",
            "        [-0.7066,  1.6546],\n",
            "        [-1.1040,  1.8525],\n",
            "        [-0.8058, -0.3335],\n",
            "        [-2.1184, -0.1623],\n",
            "        [ 0.9311,  0.5151],\n",
            "        [ 0.0515, -1.2209],\n",
            "        [ 0.6338,  0.5637],\n",
            "        [-0.5873,  0.6899],\n",
            "        [-0.2182, -1.0932],\n",
            "        [-1.8941,  0.9387],\n",
            "        [-0.2920,  0.7678],\n",
            "        [ 0.0081,  1.4718],\n",
            "        [-0.5054,  0.0856],\n",
            "        [ 1.1886,  0.2958],\n",
            "        [-1.6456, -0.2673],\n",
            "        [ 0.8625, -0.3168],\n",
            "        [-0.5383,  0.5732],\n",
            "        [ 0.5187,  1.1429],\n",
            "        [ 1.0925,  0.8887],\n",
            "        [-0.2024, -0.2036],\n",
            "        [ 0.1172,  0.7210],\n",
            "        [ 1.1309, -0.9785],\n",
            "        [ 0.0281,  2.5055],\n",
            "        [-1.0255,  0.1625],\n",
            "        [-1.3052, -1.6744],\n",
            "        [-0.6093, -0.5554],\n",
            "        [-1.5508,  0.8843],\n",
            "        [-0.7131, -0.6398],\n",
            "        [-0.0044, -2.2157],\n",
            "        [-1.6773, -0.4609],\n",
            "        [ 0.7861,  0.3075],\n",
            "        [ 0.1724, -1.1297],\n",
            "        [ 0.0850, -0.2032],\n",
            "        [ 0.7231,  0.2472],\n",
            "        [-0.9408, -0.2244],\n",
            "        [ 0.9703, -0.4641],\n",
            "        [ 1.5848,  0.5210],\n",
            "        [-0.2197,  1.2411],\n",
            "        [-0.5520,  0.3154],\n",
            "        [-0.9818, -2.5427],\n",
            "        [-1.0108, -0.2257],\n",
            "        [ 1.9699, -1.4270],\n",
            "        [ 0.3479,  0.7560],\n",
            "        [ 0.2528,  0.9146],\n",
            "        [-0.2885,  2.4468],\n",
            "        [-0.4960,  1.4369],\n",
            "        [-2.1834,  1.3173],\n",
            "        [ 0.2157,  0.4347],\n",
            "        [ 0.2528, -0.5492],\n",
            "        [ 2.0389,  0.6291],\n",
            "        [-0.8232, -0.7182],\n",
            "        [-1.5080,  0.8773],\n",
            "        [-0.4791,  1.4251],\n",
            "        [-0.2170, -0.7194],\n",
            "        [-0.2337, -0.1108],\n",
            "        [-0.0551, -2.4801],\n",
            "        [-1.3994, -1.5525],\n",
            "        [ 0.0468, -1.0005],\n",
            "        [ 1.0221, -0.4777],\n",
            "        [-1.6557, -1.1813],\n",
            "        [-0.2416,  0.8602],\n",
            "        [ 0.6669, -0.2274],\n",
            "        [-1.4364,  0.7164],\n",
            "        [-0.2714, -0.1963],\n",
            "        [ 0.5914, -0.3612],\n",
            "        [ 0.9478, -0.0804],\n",
            "        [ 0.3794,  1.2179],\n",
            "        [ 2.0056,  0.3283],\n",
            "        [ 1.0978, -0.3821],\n",
            "        [-1.4319,  1.1800],\n",
            "        [-0.1820,  0.8137],\n",
            "        [ 0.0438,  0.1533],\n",
            "        [-1.3498,  0.0196],\n",
            "        [ 0.6949, -1.5121],\n",
            "        [-0.5033, -1.5945],\n",
            "        [-0.0131,  0.3077],\n",
            "        [-1.6547, -2.1619],\n",
            "        [-0.8854, -0.2809],\n",
            "        [ 0.5329, -0.0183],\n",
            "        [ 1.4145,  1.3488],\n",
            "        [-0.0648, -0.6545],\n",
            "        [ 0.8527,  0.7121],\n",
            "        [ 0.3692, -0.4219],\n",
            "        [-0.1905, -1.9818],\n",
            "        [ 2.2705,  0.1513],\n",
            "        [-1.7230,  0.5933],\n",
            "        [ 0.9256, -1.2029],\n",
            "        [-1.3732, -1.5373],\n",
            "        [-0.2699,  0.9437],\n",
            "        [ 0.3655,  0.6089],\n",
            "        [ 0.1238,  1.1526],\n",
            "        [-0.2486,  0.3702],\n",
            "        [-1.3060,  0.7707],\n",
            "        [-1.5869,  1.9134],\n",
            "        [ 0.1851,  0.4158],\n",
            "        [-1.7332, -0.2599],\n",
            "        [ 0.4224, -0.0202],\n",
            "        [-2.0461, -0.3806],\n",
            "        [ 0.2792, -1.4163],\n",
            "        [-0.0413, -1.2202],\n",
            "        [ 0.1048,  0.4732],\n",
            "        [-1.6188,  1.6640],\n",
            "        [ 1.0785,  1.1441],\n",
            "        [-1.6445, -0.5646],\n",
            "        [ 1.6704, -0.0272],\n",
            "        [ 2.5186,  0.9531],\n",
            "        [ 0.4758, -2.4222],\n",
            "        [ 0.8840, -0.1823],\n",
            "        [-1.6686,  0.3543],\n",
            "        [ 0.5397,  0.5262],\n",
            "        [ 0.6165,  0.2992],\n",
            "        [ 0.7342, -1.4012],\n",
            "        [ 0.7115,  0.0293],\n",
            "        [ 0.7584,  0.0300],\n",
            "        [-0.3410, -0.9432],\n",
            "        [ 1.3501, -1.7728],\n",
            "        [-0.9429, -1.5699],\n",
            "        [ 2.5328, -1.4086],\n",
            "        [-0.2177, -0.7305],\n",
            "        [-1.0322,  0.5607],\n",
            "        [ 1.3480,  0.1333],\n",
            "        [ 0.2114,  0.3244],\n",
            "        [ 1.0854,  0.2310],\n",
            "        [ 1.1090,  0.5149],\n",
            "        [-1.6817,  1.4650],\n",
            "        [ 0.7755,  1.0974],\n",
            "        [-0.7343, -0.2493],\n",
            "        [-0.7019,  0.7370],\n",
            "        [-0.9437, -0.9149],\n",
            "        [-0.2220, -0.2830],\n",
            "        [ 2.3317,  1.5967],\n",
            "        [ 0.3200, -0.2156],\n",
            "        [ 0.1144, -0.7709],\n",
            "        [-1.6620, -1.2186],\n",
            "        [ 0.7825, -1.1143],\n",
            "        [ 1.3089, -0.3498],\n",
            "        [-1.5753, -0.8236],\n",
            "        [-0.6417,  1.9345],\n",
            "        [-2.0542,  0.4923],\n",
            "        [ 0.3017, -2.3559],\n",
            "        [ 1.7506,  0.0524],\n",
            "        [ 1.5176, -1.0385],\n",
            "        [ 2.6427, -1.3919],\n",
            "        [ 0.7232,  1.1937],\n",
            "        [-0.2078,  1.1065],\n",
            "        [-0.5148,  0.3786],\n",
            "        [ 0.0561, -1.0932],\n",
            "        [ 0.9774, -0.9340],\n",
            "        [-1.6065,  0.0822],\n",
            "        [ 1.7953,  0.1059],\n",
            "        [-0.9172, -0.9626],\n",
            "        [-0.3412, -0.7216],\n",
            "        [ 0.5722,  0.5361],\n",
            "        [ 1.2055,  0.1025],\n",
            "        [ 0.0607, -0.7509],\n",
            "        [ 0.0436,  0.5565],\n",
            "        [-0.2570, -0.2651],\n",
            "        [ 2.2139, -0.1707],\n",
            "        [ 0.0808, -0.8698],\n",
            "        [-0.1149, -0.5118],\n",
            "        [-2.1261,  1.1739],\n",
            "        [-2.2222, -0.9122],\n",
            "        [ 0.2864,  1.9205],\n",
            "        [ 0.6534,  0.5998],\n",
            "        [ 0.6082, -0.2142],\n",
            "        [-1.7126,  0.1258],\n",
            "        [-1.0573, -2.0843],\n",
            "        [-0.5531,  0.0527],\n",
            "        [-1.0771,  0.2317],\n",
            "        [ 0.9460, -0.5597],\n",
            "        [-1.8494,  0.0319],\n",
            "        [ 0.0950, -1.6008],\n",
            "        [ 0.0858, -0.6240],\n",
            "        [-1.0396, -0.1980],\n",
            "        [ 0.9291,  0.2509],\n",
            "        [ 1.3087, -0.4443],\n",
            "        [ 0.0570,  2.1989],\n",
            "        [-0.5693, -0.1302],\n",
            "        [-0.1135,  0.4979],\n",
            "        [ 0.4352,  0.5883],\n",
            "        [-1.1060, -0.2179],\n",
            "        [ 1.3900, -0.3167],\n",
            "        [-0.1574,  1.4836],\n",
            "        [ 1.9507, -0.3988],\n",
            "        [-0.1508,  0.0808],\n",
            "        [ 1.3464,  0.1276],\n",
            "        [-1.3828, -1.5305],\n",
            "        [-0.3722,  0.5959],\n",
            "        [ 0.0347, -0.4395],\n",
            "        [ 2.7098,  0.6725],\n",
            "        [ 0.2390, -0.6945]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Next_Word_Predictor(nn.Module):\n",
        "  def __init__(self, block_size, vocab_size, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    # Input size: vocab_size (the total number of characters in the vocabulary).\n",
        "    # Output size: emb_dim (the size of the dense vector representation for each character).\n",
        "    self.emb=nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    # Input size: block_size * emb_dim\n",
        "    # Output size: hidden_dim (the size of the hidden layer).\n",
        "    self.linear1=nn.Linear(block_size*emb_dim, hidden_dim)\n",
        "\n",
        "    # Input size: hidden_dim\n",
        "    # Output size: vocab_size (the total number of words in the vocabulary).\n",
        "    self.linear2=nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # I/P layer\n",
        "    x = self.emb(x)\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    # Hidden layer\n",
        "    x = self.linear1(x)\n",
        "    x = torch.relu(x)\n",
        "    # Output layer\n",
        "    x = self.linear2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "TlE3TDmUIQFh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate names from untrained model\n",
        "\n",
        "\n",
        "model=Next_Word_Predictor(block_size, len(stoi), emb_dim, 100).to(device)\n",
        "model=torch.compile(model)\n",
        "\n",
        "def generate_next_word(model, itos, stoi, block_size, max_len=10):\n",
        "  context=input(\"Enter the sentence: \")\n",
        "  context=re.sub('[^a-zA-Z0-9 \\.]', '', context)\n",
        "  context = [stoi[word] for word in context.split()]\n",
        "\n",
        "  # Adjust the length of context\n",
        "  if len(context)<block_size:\n",
        "      # Prepend zeros if the length is less than context_length\n",
        "      context=[0]*(block_size-len(context))+context\n",
        "  elif len(context)>block_size:\n",
        "      # Take the last context_length words if the length is greater than context_length\n",
        "      context=context[-block_size:]\n",
        "\n",
        "  x=torch.tensor(context).view(1, -1).to(device)\n",
        "  y_pred=model(x)\n",
        "  ix=torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n",
        "  word=itos[ix]\n",
        "  return word\n",
        "\n",
        "print(generate_next_word(model, itos, stoi, block_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEvHmhNz4-EM",
        "outputId": "91a605c0-ccaa-4e29-e2c3-18191ac12237"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence: you have\n",
            "call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param_name, param in model.named_parameters():\n",
        "    print(param_name, param.shape)"
      ],
      "metadata": {
        "id": "SVQiDEBT9dO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Train the model\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "\n",
        "# Mini-batch training\n",
        "batch_size = 4096\n",
        "print_every = 100\n",
        "elapsed_time = []\n",
        "for epoch in range(2000):\n",
        "    start_time = time.time()\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i+batch_size]\n",
        "        y = Y[i:i+batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    end_time = time.time()\n",
        "    elapsed_time.append(end_time - start_time)\n",
        "    if epoch % print_every == 0:\n",
        "        print(epoch, loss.item())"
      ],
      "metadata": {
        "id": "DaGFXWFn97ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embedding\n",
        "\n",
        "plot_emb(model.emb, itos)"
      ],
      "metadata": {
        "id": "-p1gh1SqJ7x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate names from trained model\n",
        "\n",
        "for i in range(1):\n",
        "    print(generate_next_word(model, itos, stoi, block_size))"
      ],
      "metadata": {
        "id": "QhoulG5QKPuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}